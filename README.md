# Minimal RAG Prototype

This is a minimal Retrieval-Augmented Generation (RAG) prototype.  
It loads text documents, stores their embeddings in a vector database, and answers user queries using a language model (LLM) with retrieved context.  

You can run it in two main ways:
1. **Backend (FastAPI) + Frontend (React)** â†’ Recommended for UI  
2. **CLI Mode** â†’ Simple terminal chatbot  

---

## ðŸš€ Features

- Load and embed `.txt` files  
- Split documents into chunks (paragraph + recursive splitting)  
- Store and query embeddings in a vector store  
- Answer questions using an open-source Hugging Face LLM  
- Return answers **with source document names**  
- Simple **React frontend** and **FastAPI backend**  

---

**##for setup instructions go to setup_instructions.txt file in the repo**
